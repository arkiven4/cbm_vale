{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990f6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"ipykernel_launcher.py\",  \n",
    "    \"--dataset\", \"CustomAWGN30ES15\",  # CustomAWGN30ES15 #Custom2024AWGN30ES15 LGS3202020253AWGN30ES15\n",
    "    \"--model\", \"\",\n",
    "    \"--Device\", \"cpu\",\n",
    "    \"--test\"  # This is a flag, so no value needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f8267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "import src.commons as commons\n",
    "import src.custom_const as custom_const\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ebc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = ['Active Power', 'Reactive Power', 'Governor speed actual', 'UGB X displacement', \n",
    "               'UGB Y displacement', 'LGB X displacement', 'LGB Y displacement', 'TGB X displacement',\n",
    "               'TGB Y displacement', 'Stator winding temperature 13',\n",
    "               'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "               'Surface Air Cooler Air Outlet Temperature',\n",
    "               'Surface Air Cooler Water Inlet Temperature',\n",
    "               'Surface Air Cooler Water Outlet Temperature',\n",
    "               'Stator core temperature', 'UGB metal temperature',\n",
    "               'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "               'LGB oil temperature', 'Penstock Flow', 'Turbine flow',\n",
    "               'UGB cooling water flow', 'LGB cooling water flow',\n",
    "               'Generator cooling water flow', 'Governor Penstock Pressure',\n",
    "               'Penstock pressure', 'Opening Wicked Gate', 'UGB Oil Contaminant',\n",
    "               'Gen Thrust Bearing Oil Contaminant']\n",
    "reverse_mapping = {v: k for k, v in custom_const.feature_tag_mapping.items() if k in feature_set + ['Grid Selection']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5869cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "master_pd = \"\"\n",
    "column_name = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"data_2025/2025\"):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        tag_name = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "        feature_key = reverse_mapping.get(tag_name)\n",
    "        if feature_key != None:\n",
    "            column_name.append(feature_key)\n",
    "\n",
    "            value_resp = pd.read_csv(filepath)\n",
    "            if count == 0:\n",
    "                value_resp['Timestamps'] = pd.to_datetime(value_resp['Timestamps'])\n",
    "                master_pd = value_resp\n",
    "            else:\n",
    "                master_pd = pd.concat([master_pd, value_resp['Values']], axis=1, join='inner')\n",
    "\n",
    "            count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0deeaa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_pd = master_pd.values\n",
    "master_pd = pd.DataFrame(data=master_pd, columns=['TimeStamp'] + list(column_name))\n",
    "master_pd = master_pd.reset_index(drop=True)\n",
    "master_pd.replace('I/O Timeout', np.nan, inplace=True)\n",
    "master_pd.replace('No Data', np.nan, inplace=True)\n",
    "master_pd.replace('Future Data Unsupported', np.nan, inplace=True)\n",
    "master_pd.replace('Closed', np.nan, inplace=True)\n",
    "master_pd.replace('Open', np.nan, inplace=True)\n",
    "master_pd = master_pd.sort_values(by='TimeStamp')\n",
    "master_pd = master_pd.reset_index(drop=True)\n",
    "master_pd = master_pd.fillna(method='ffill')\n",
    "master_pd = master_pd[['TimeStamp'] + feature_set + ['Grid Selection']]\n",
    "for column_name in master_pd.columns:\n",
    "    if column_name != 'Load_Type' and column_name != 'TimeStamp':\n",
    "        master_pd[column_name] = pd.to_numeric(master_pd[column_name], downcast='float')\n",
    "\n",
    "df_data_withtime = master_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0caec80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, sqlite3, copy, time, sklearn, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from scipy.signal import resample\n",
    "\n",
    "from src.models import *\n",
    "from src.utils import *\n",
    "from main import  load_dataset, backprop\n",
    "import src.commons as commons\n",
    "import src.custom_const as custom_const\n",
    "from src.commons import OnlinePercentileEstimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5966728",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_array = [\"Attention\", \"DTAAD\", \"MAD_GAN\", \"TranAD\", \"DAGMM\", \"USAD\", \"OmniAnomaly\"]\n",
    "\n",
    "commons.init_db_timeconst(feature_set, \"db/original_data.db\", \"original_data\")\n",
    "commons.init_db_timeconst(['Grid Selection'], \"db/original_data.db\", \"additional_original_data\")\n",
    "commons.init_db_timeconst(feature_set, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "commons.init_db_timeconst(feature_set, \"db/severity_trendings.db\", \"original_sensor\")\n",
    "for model_name in model_array:\n",
    "    commons.init_db_timeconst(feature_set, \"db/pred_data.db\", model_name)\n",
    "    commons.init_db_timeconst(feature_set, \"db/threshold_data.db\", model_name)\n",
    "    commons.init_db_timeconst(feature_set, \"db/adaptive_tdigest.db\", model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0382068",
   "metadata": {},
   "outputs": [],
   "source": [
    "measured_horizon = 60 * 2 * 1 # Minute\n",
    "interval_gap = 90\n",
    "\n",
    "# Update Thr Each\n",
    "total_days = 30\n",
    "total_minutes = total_days * 24 * 60\n",
    "count_accumulateArrayForCalc = total_minutes * 60 // interval_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8423ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('normalize_2023.pickle', 'rb') as handle:\n",
    "    normalize_obj = pickle.load(handle)\n",
    "    min_a, max_a = normalize_obj['min_a'], normalize_obj['max_a']\n",
    "\n",
    "tdigest_models = {}\n",
    "for model_now in model_array:\n",
    "    path_try = f'mini_loss_fold/{args.dataset}/{model_now}_tdigest_run.pickle'\n",
    "    path_fallback = f'mini_loss_fold/{args.dataset}/{model_now}_tdigest.pickle'\n",
    "    with open(path_try if os.path.exists(path_try) else path_fallback, 'rb') as handle:\n",
    "        tdigest_models[model_now] = pickle.load(handle)\n",
    "\n",
    "loss_accumulative_path = \"mini_loss_fold/loss_accumulative.pickle\"\n",
    "if os.path.exists(loss_accumulative_path):\n",
    "    with open(loss_accumulative_path, 'rb') as handle:\n",
    "        loss_accumulative = pickle.load(handle)\n",
    "else:\n",
    "    loss_accumulative = {model_now: [[] for _ in feature_set] for model_now in model_array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a1d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 3443/3443 [5:11:21<00:00,  5.43s/it]  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_timestamp_last = np.datetime64('2025-01-01 20:00:00') #2023-09-04T15:55:00\n",
    "end_date_filter = pd.to_datetime('2025-08-04 23:59:00')\n",
    "start_trend_filter = pd.to_datetime('2025-01-01 20:00:00')\n",
    "current_end_window = start_trend_filter\n",
    "total_steps = int((end_date_filter - current_end_window).total_seconds() // (interval_gap * 60)) + 1\n",
    "df_timestamp_last = None\n",
    "for step in tqdm(range(total_steps), desc=\"Progress\"):\n",
    "    valid_measurment = True\n",
    "    threshold_percentages = {}\n",
    "    ypred_models = {}\n",
    "\n",
    "    start_date_window = current_end_window - timedelta(minutes=measured_horizon)\n",
    "    mask = (df_data_withtime['TimeStamp'] > start_date_window.strftime('%Y-%m-%d %H:%M:%S')) & (df_data_withtime['TimeStamp'] <= current_end_window.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    df_sel = df_data_withtime.loc[mask].iloc[:120, :]\n",
    "    df_additional = df_sel[['Grid Selection']].copy()\n",
    "    df_additional = df_additional.astype(float)\n",
    "    df_sel = df_sel[['TimeStamp'] + feature_set] \n",
    "    if len(df_sel) <= 0:\n",
    "        current_end_window += timedelta(minutes=interval_gap)\n",
    "        continue\n",
    "    \n",
    "    load_label = df_sel.apply(commons.label_load, axis=1).value_counts()\n",
    "    bad_pct = (load_label.get('No Load', 0) +  load_label.get('Shutdown', 0)) / load_label.sum()\n",
    "    testD, testO, df_timestamp, df_feature = commons.preprocessPD_loadData(df_sel, feature_set, min_a, max_a)\n",
    "    for feat_index, feature_now in enumerate(feature_set):\n",
    "        if (df_feature[feature_set] == 0).any().any():\n",
    "            valid_measurment = False\n",
    "\n",
    "    temploss_models = {model_now: None for model_now in model_array}\n",
    "    for model_now in model_array:\n",
    "        model = commons.load_model(args.dataset, model_now, testO.shape[1], args.retrain, args.test)\n",
    "        model.eval()\n",
    "        if model.name in ['Attention', 'DAGMM', 'USAD', 'MSCRED', 'CAE_M', 'GDN', 'MTAD_GAT', 'MAD_GAN', 'TranAD'] or 'DTAAD' in model.name:\n",
    "            testD_now = commons.convert_to_windows(testD, model)\n",
    "        else:\n",
    "            testD_now = testD\n",
    "        loss, y_pred = backprop(0, model, testD_now, testO, None, None, training=False)\n",
    "        y_pred = np.where(np.isfinite(y_pred), y_pred, np.finfo(np.float32).eps)\n",
    "        temploss_models[model_now] = loss\n",
    "        ypred_models[model_now] = commons.denormalize3(y_pred, min_a, max_a)\n",
    "        threshold_percentages[model_now] = commons.calcThres_oneModel(feature_set, loss, tdigest_models[model_now])\n",
    "\n",
    "    counter_feature_trd, _ = commons.calc_counterPercentage(threshold_percentages, feature_set, model_array)\n",
    "    if valid_measurment and bad_pct == 0.0:\n",
    "        for feat_index, feature_now in enumerate(feature_set):\n",
    "            if counter_feature_trd[feature_now]['percentage'] <= 1.0 and counter_feature_trd[feature_now]['count'] == 0:\n",
    "                for model_now in model_array:\n",
    "                    loss_accumulative[model_now][feat_index].append(temploss_models[model_now][:, feat_index])\n",
    "\n",
    "    df_feature = commons.denormalize3(df_feature, min_a, max_a)\n",
    "    df_feature_mean = commons.trunc(df_feature.values.mean(axis=0), decs=2)\n",
    "    df_feature = df_feature.values[::6]\n",
    "    df_additional = df_additional.values[::6]\n",
    "    df_timestamp = df_timestamp.dt.floor(\"min\")[::6].values[:20]\n",
    "    for model_now in model_array:\n",
    "        ypred_models[model_now] = ypred_models[model_now][::6]\n",
    "\n",
    "    min_len = min(len(df_timestamp), len(df_feature), *map(len, ypred_models.values()))\n",
    "    df_timestamp = df_timestamp[:min_len]\n",
    "    df_feature = df_feature[:min_len]\n",
    "    df_additional = df_additional[:min_len]\n",
    "    for model_now in model_array:\n",
    "        ypred_models[model_now] = ypred_models[model_now][:min_len]\n",
    "\n",
    "    if df_timestamp_last is not None:\n",
    "        mask = df_timestamp > df_timestamp_last\n",
    "    else:\n",
    "        mask = pd.Series([True] * len(df_timestamp))\n",
    "    df_feature = df_feature[mask]\n",
    "    df_additional = df_additional[mask]\n",
    "    df_timestamp = df_timestamp[mask]\n",
    "    for model_now in model_array:\n",
    "        ypred_models[model_now] = ypred_models[model_now][mask]\n",
    "\n",
    "    if len(df_timestamp) <= 0:\n",
    "        count = count + 1\n",
    "        time.sleep(interval_gap)\n",
    "        continue\n",
    "\n",
    "    df_timestampi = pd.to_datetime(df_timestamp[-1])\n",
    "    trend_data = np.array([counter_feature_trd[key]['percentage'] for key in counter_feature_trd]).astype(np.float64)\n",
    "\n",
    "    commons.batch_timeseries_savedb(df_timestamp, commons.trunc(df_feature, decs=2), feature_set, \"db/original_data.db\", \"original_data\")\n",
    "    commons.batch_timeseries_savedb(df_timestamp, commons.trunc(df_additional, decs=2), ['Grid Selection'], \"db/original_data.db\", \"additional_original_data\")\n",
    "    commons.timeseries_savedb(df_timestampi, trend_data, feature_set, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "    commons.timeseries_savedb(df_timestampi, df_feature_mean, feature_set, \"db/severity_trendings.db\", \"original_sensor\")\n",
    "    for idx_model, (model_name) in enumerate(model_array):\n",
    "        commons.batch_timeseries_savedb(df_timestamp, commons.trunc(ypred_models[model_name], decs=2), feature_set, \"db/pred_data.db\", model_name)\n",
    "        commons.timeseries_savedb(df_timestampi, commons.trunc(np.array(list(threshold_percentages[model_name].values())), decs=2), feature_set, \"db/threshold_data.db\", model_name)\n",
    "        commons.timeseries_savedb(df_timestampi, commons.trunc(np.array([tdigest_models[model_name][index].get_percentile(99) for index in range(len(feature_set))]), decs=6), feature_set, \"db/adaptive_tdigest.db\", model_name)\n",
    "\n",
    "    for model_now in model_array:\n",
    "        for feat_index in range(len(feature_set)):\n",
    "            if len(loss_accumulative[model_now][feat_index]) >= count_accumulateArrayForCalc:\n",
    "                value_toupdate = np.concatenate(loss_accumulative[model_now][feat_index], axis=0)\n",
    "                tdigest_models[model_now][feat_index].update(value_toupdate)\n",
    "                loss_accumulative[model_now][feat_index] = []\n",
    "        with open(f'mini_loss_fold/{args.dataset}/{model_now}_tdigest_run.pickle', 'wb') as handle:\n",
    "            pickle.dump(tdigest_models[model_now], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open(f'mini_loss_fold/loss_accumulative.pickle', 'wb') as handle:\n",
    "        pickle.dump(loss_accumulative, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # DONT REMOVE THIS\n",
    "    df_timestamp_last = df_timestamp[-1]\n",
    "    current_end_window += timedelta(minutes=interval_gap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
